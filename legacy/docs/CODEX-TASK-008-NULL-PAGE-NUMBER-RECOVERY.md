# Codex Task 008: Pre-Validation Recovery for Null Top-Level Fields

## Context

When the vision model returns `page_number: null` (instead of the correct integer), the tile fails permanently with `status: "validation_error"` and all extracted data is lost. This is a gap in the existing two-pass validation recovery system.

**Evidence from corridor-expanded run:**

`output/extractions/corridor-expanded/p26_r1_c0.json.meta.json`:
```json
{
  "status": "validation_error",
  "tile_id": "p26_r1_c0",
  "error": "1 validation error for TileExtraction\npage_number\n  Input should be a valid integer [type=int_type, input_value=None, input_type=NoneType]"
}
```

The raw model output (`p26_r1_c0.json.raw.txt`) shows a coherence score of 0.98 and valid structural content (water tee + gate valve at STA 20+00 on page 26 profile view). The data is recoverable — the only flaw is `page_number: null`.

---

## Root Cause

`run_hybrid_extraction()` in `src/extraction/run_hybrid.py` has two validation passes (lines 558–592):

```
Pass 1: TileExtraction.model_validate(payload_obj)
           ↓ fails on page_number=null
Pass 2: _sanitize_extraction_payload(payload_obj)
        TileExtraction.model_validate(sanitized_payload)
           ↓ still fails — sanitizer only touches structures/pipes/callouts,
             not top-level fields like page_number
```

After both passes fail, escalation is attempted. When escalation is unavailable or also fails, the tile is written with `status: "validation_error"` and no `.json` output is produced — the tile is permanently excluded from graph assembly.

The **correct authoritative values** for `tile_id` and `page_number` already exist in the `text_layer` dict (loaded at line 350). The existing post-validation correction at lines 599–611 uses exactly this source to fix wrong-but-valid values (e.g., model returning `page_number=14` when the tile is actually page 26). The problem is that null values cause Pydantic to reject the payload *before* this correction can run.

**Comparison:**

| Tile | `page_number` from model | Outcome |
|---|---|---|
| `p26_r1_c1` | `14` (wrong integer) | Pydantic accepts it → post-validation corrects to `26` → `status: ok` |
| `p26_r1_c0` | `null` | Pydantic rejects → sanitizer can't fix it → `status: validation_error` |

---

## Implementation

### Part 1: Pre-Validation Metadata Patch — `src/extraction/run_hybrid.py`

Add a new helper function `_pre_correct_tile_metadata` that patches null or missing top-level identity fields using the authoritative text_layer values, **before** any Pydantic validation attempt:

```python
def _pre_correct_tile_metadata(payload: dict[str, Any], text_layer: dict[str, Any]) -> None:
    """
    Patch null/missing tile_id and page_number from the authoritative text layer.

    The text layer is generated by the tiler and is always correct. The vision
    model occasionally returns null for these fields, which would cause Pydantic
    validation to fail before the sanitizer gets a chance to run. This function
    fixes those nulls in-place so validation can proceed.
    """
    if not payload.get("tile_id"):
        authoritative = str(text_layer.get("tile_id", "")).strip()
        if authoritative:
            payload["tile_id"] = authoritative

    if payload.get("page_number") is None:
        raw = text_layer.get("page_number")
        if raw is not None:
            try:
                payload["page_number"] = int(raw)
            except (TypeError, ValueError):
                pass
```

**Placement:** Add this function near the other private helpers, before `run_hybrid_extraction()`.

**Call site:** Call it immediately before the first `TileExtraction.model_validate()` attempt, at line 558 (just before the `try` block for pass 1):

Current code:
```python
sanitized = False
dropped_invalid_counts = {"structures": 0, "inverts": 0, "pipes": 0, "callouts": 0}
try:
    extraction = TileExtraction.model_validate(payload_obj)
```

Change to:
```python
sanitized = False
dropped_invalid_counts = {"structures": 0, "inverts": 0, "pipes": 0, "callouts": 0}
_pre_correct_tile_metadata(payload_obj, text_layer)
try:
    extraction = TileExtraction.model_validate(payload_obj)
```

That's the entire code change. One function, one call site.

**Why this is safe:**
- `text_layer` is loaded from disk at the top of `run_hybrid_extraction()` and always reflects the tiler's authoritative tile_id and page_number — these never come from the model.
- The existing post-validation correction (lines 599–611) continues to work as before. If the model returned a wrong integer (e.g., `14` instead of `26`), the pre-correction doesn't fire (since `page_number` is not null) and the post-validation correction handles it exactly as today.
- If the model returned null for both `tile_id` and `page_number`, both are patched from the text_layer before validation.
- The function modifies `payload_obj` in-place. `payload_obj` is only used for Pydantic validation; the raw model output has already been written to `raw_output_path` at line 511, so no data is lost.

---

## Validation

### Step 1: Run unit tests

```bash
python -m pytest tests/ -v
```

All existing tests must pass. No new tests are required for this change (the fix is a one-liner in an integration code path), but if you want to add a unit test, test that `_pre_correct_tile_metadata` correctly patches `page_number=None` from the text_layer, and does NOT overwrite a valid integer.

### Step 2: Re-run the failed tile

The failed tile (`p26_r1_c0`) has `status: "validation_error"` in its meta, so it will be re-processed without `--no-cache` (the cache check only returns early for `status: "ok"` or `"dry_run"`). All other 29 tiles will use their cached results.

First confirm the text_layers directory for the corridor-expanded run. It is typically at:
```
output/text_layers/corridor-expanded/
```

Check that `p26_r1_c0.json` exists there (should have `page_number: 26`). Then re-run the batch:

```bash
python -m src.extraction.run_hybrid_batch \
  --tiles-dir output/tiles/corridor-expanded \
  --text-layers-dir output/text_layers/corridor-expanded \
  --out-dir output/extractions/corridor-expanded
```

Only `p26_r1_c0` should re-process; the other 29 tiles return immediately from cache.

### Step 3: Verify the recovered tile

Check that:
- `output/extractions/corridor-expanded/p26_r1_c0.json.meta.json` now shows `status: "ok"` (or `"sanitized"`)
- `output/extractions/corridor-expanded/p26_r1_c0.json` exists and has `page_number: 26`

### Step 4: Regenerate corridor-expanded findings and report

```bash
python -c "
import json
from pathlib import Path
from src.graph.assembly import build_utility_graph, load_extractions_with_meta
from src.graph.checks import run_all_checks

extractions_dir = Path('output/extractions/corridor-expanded')
findings_dir = Path('output/graphs/findings')
findings_dir.mkdir(parents=True, exist_ok=True)
prefix = 'corridor-expanded'

extractions, tile_meta = load_extractions_with_meta(extractions_dir)
for utility in ('SD', 'SS', 'W'):
    graph = build_utility_graph(extractions=extractions, utility_type=utility, tile_meta_by_id=tile_meta)
    findings = run_all_checks(graph)
    payload = {
        'utility_type': utility,
        'graph': {
            'nodes': graph.number_of_nodes(),
            'edges': graph.number_of_edges(),
            'quality_summary': graph.graph.get('quality_summary', {}),
        },
        'counts': {
            'total_findings': len(findings),
            'by_severity': {},
            'by_type': {},
        },
        'findings': [f.to_dict() for f in findings],
    }
    for f in findings:
        payload['counts']['by_severity'][f.severity] = payload['counts']['by_severity'].get(f.severity, 0) + 1
        payload['counts']['by_type'][f.finding_type] = payload['counts']['by_type'].get(f.finding_type, 0) + 1
    out_path = findings_dir / f'{prefix}-{utility.lower()}-findings.json'
    out_path.write_text(json.dumps(payload, indent=2), encoding='utf-8')
    print(f'{utility}: {len(findings)} findings -> {out_path}')
"

python -m src.report.html_report \
  --graphs-dir output/graphs \
  --findings-dir output/graphs/findings \
  --prefix corridor-expanded \
  --batch-summary output/extractions/corridor-expanded/batch_summary.json \
  --out output/reports/corridor-expanded-report.html
```

---

## Files to Modify

1. **`src/extraction/run_hybrid.py`** — Add `_pre_correct_tile_metadata()` helper; call it before the first `model_validate()` attempt

## Files NOT to Modify

- `src/extraction/schemas.py` — Do NOT make `page_number` Optional in the Pydantic model. It is legitimately required (assembly breaks without it). The fix should happen before validation, not by weakening the schema.
- `src/graph/assembly.py` — No changes needed
- `src/graph/checks.py` — No changes needed
- `tests/test_graph_checks.py` — No changes needed

---

## Success Criteria

1. `p26_r1_c0.json.meta.json` status changes from `"validation_error"` to `"ok"` after re-running the batch
2. `p26_r1_c0.json` is present with `page_number: 26` and valid content
3. Corridor-expanded batch summary shows 30 OK tiles (was 29 OK, 1 validation_error)
4. All existing unit tests pass
5. Quality grade may improve from D (was 23/30 sanitized, 1 validation_error) as the recovered tile adds a clean extraction
