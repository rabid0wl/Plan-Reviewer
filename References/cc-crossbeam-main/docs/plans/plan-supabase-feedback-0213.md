Good plan overall. Let me address your specific concern about the schema and the contractor questions flow.

Schema Review: Does It Handle the Contractor Questions Loop?

The schema as written has a gap. Here's the issue:

The contractor corrections flow isn't a single fire-and-forget run like the city review flow. It's a two-phase flow with a human-in-the-loop pause:

Phase 1 (Skill 1): Agent analyzes corrections → generates contractor_questions.json
                    ↓
         ⏸️ PAUSE — waiting for contractor/judge to ANSWER the questions
                    ↓
Phase 2 (Skill 2): Agent reads answers → generates response package
Your corrections-analysis.ts runs Skill 1, and corrections-response.ts runs Skill 2. They're separate query() calls. Between them, the contractor needs to see the questions and provide answers.

What the current schema is missing:

1. contractor_answers table or column — Where do the user's answers get stored before Phase 2 runs?
2. Multi-status tracking — The project needs states like 'awaiting-answers' between phases, not just ready → processing → completed
3. Which phase is running? — When you hit processing, is it Phase 1 or Phase 2?
Proposed Schema Fix

-- Update projects table status options:
CREATE TABLE crossbeam.projects (
  ...
  status TEXT NOT NULL DEFAULT 'ready'
    CHECK (status IN (
      'ready',
      'uploading',
      'processing-phase1',    -- ← Skill 1 running (corrections analysis)
      'awaiting-answers',     -- ← Skill 1 done, waiting for contractor input
      'processing-phase2',    -- ← Skill 2 running (response generation)
      'processing',           -- ← City review (single phase)
      'completed',
      'failed'
    )),
  ...
);

-- Add contractor questions/answers table
CREATE TABLE crossbeam.contractor_answers (
  id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
  project_id UUID REFERENCES crossbeam.projects(id) ON DELETE CASCADE NOT NULL,
  question_key TEXT NOT NULL,        -- matches key from contractor_questions.json
  question_text TEXT NOT NULL,       -- the question as generated by Skill 1
  answer_text TEXT,                  -- contractor's response (NULL until answered)
  is_answered BOOLEAN DEFAULT false,
  created_at TIMESTAMPTZ DEFAULT now(),
  updated_at TIMESTAMPTZ DEFAULT now()
);

ALTER TABLE crossbeam.contractor_answers ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can CRUD own contractor answers"
  ON crossbeam.contractor_answers FOR ALL
  USING (
    EXISTS (
      SELECT 1 FROM crossbeam.projects p
      WHERE p.id = project_id AND (p.user_id = auth.uid() OR p.is_demo = true)
    )
  );
How the Contractor Flow Works End-to-End

1. Judge clicks "Analyze Corrections" on contractor demo project
   └→ Frontend POSTs to /api/generate with flow_type: 'corrections-analysis'
   └→ Cloud Run sets status: 'processing-phase1'
   └→ Cloud Run creates sandbox, runs Skill 1 (corrections-analysis.ts)
   └→ Agent streams messages to crossbeam.messages
   └→ Skill 1 finishes → produces contractor_questions.json
   └→ Cloud Run parses contractor_questions.json
   └→ Cloud Run inserts questions into crossbeam.contractor_answers
   └→ Cloud Run sets status: 'awaiting-answers'
   └→ Frontend detects status change → shows questions form

2. Judge answers the contractor questions in the UI
   └→ Frontend updates crossbeam.contractor_answers rows
   └→ Judge clicks "Submit Answers & Generate Response"
   └→ Frontend POSTs to /api/generate with flow_type: 'corrections-response'
   └→ Cloud Run sets status: 'processing-phase2'
   └→ Cloud Run creates NEW sandbox, loads answers + Phase 1 outputs
   └→ Runs Skill 2 (corrections-response.ts)
   └→ Agent streams messages
   └→ Skill 2 finishes → produces response package
   └→ Cloud Run sets status: 'completed'
   └→ Frontend shows results

3. Judge sees: corrections letter + checklist + scope of work
What This Means for the Frontend

The project detail page needs an additional state:
[4:56 PM]| Status            | What the UI Shows                                                 |
| ----------------- | ----------------------------------------------------------------- |
| `ready`             | File preview + "Start Analysis" button                            |
| `processing-phase1` | Agent working screen + message stream                             |
| `awaiting-answers`  | *Contractor questions form* — questions from Skill 1, answer fields |
| `processing-phase2` | Agent working screen + message stream (round 2)                   |
| `processing`        | Agent working screen (city review — single phase)                 |
| `completed`         | Results viewer                                                    |
| `failed`            | Error message                                                     |
City Review Flow Is Simpler

City review is one shot: ready → processing → completed. No questions loop. The schema handles this fine as-is.

My Recommendation

Update the schema in plan-strategy-0213 before executing Stream 0. The changes are:

1. Expand the status CHECK constraint to include processing-phase1, awaiting-answers, processing-phase2
2. Add the crossbeam.contractor_answers table
3. The outputs table is fine — it stores the final results from whichever phase produces them
For the hackathon demo, you could simplify by pre-filling the contractor answers (skip the questions form entirely — judge sees Skill 1 run, then immediately Skill 2 runs with canned answers). But having the schema support it means you can build the real UI if time allows.

One more thought: The outputs table should probably also store the Phase 1 intermediate outputs (the 8 JSON files from Skill 1) since Skill 2 needs them. Either add columns for those or use a JSONB phase1_artifacts column as a catch-all.

Want me to update plan-deploy.md and the strategy file with these schema changes?

------

Alright, I've got the full picture now. Yes, I have Supabase MCP connected and working -- I just pulled the live schema. Here's my honest assessment.
The Short Answer
The proposed schema will work, but it's lossy. It's a good 80% fit for a hackathon demo. There are a few gaps worth knowing about before you nuke and rebuild.
What I Found
The agents SDK code has ZERO Supabase references
The entire agents-crossbeam/ codebase is file-based. It writes everything to local sessions/ directories as JSON and Markdown files. There's no Supabase client, no .from(), no .insert() -- nothing. The only env var it needs is ANTHROPIC_API_KEY.
This means the server layer (Stream 1) is the bridge -- it runs the agent in a Vercel Sandbox, extracts the file outputs, and writes them to Supabase. So the real question is: does the proposed schema have the right columns to hold what the agent produces?
Output Mismatch: Agent produces ~13 files, schema has ~6 columns
Here's the gap. The corrections flow produces these files:
Agent Output File	Proposed outputs Column	Status
corrections_parsed.json	corrections_analysis_json	Fits (JSONB)
corrections_categorized.json	corrections_analysis_json	Conflict -- same column
contractor_questions.json	contractor_questions_json	Fits
response_letter.md	response_letter_md	Fits
professional_scope.md	action_items_json	Mismatch (MD vs JSON)
corrections_report.md	no column	Missing
sheet-manifest.json	no column	Missing
state_law_findings.json	no column	Missing
city_research_findings.json	no column	Missing
sheet_observations.json	no column	Missing
sheet_annotations.json	no column	Missing
contractor_answers.json	no column	Missing
The plan review flow has similar gaps:
Agent Output File	Proposed outputs Column	Status
draft_corrections.md	corrections_letter_md	Fits
corrections_letter.pdf	corrections_letter_pdf_path	Fits
draft_corrections.json	review_checklist_json	Fits
review_summary.json	no column	Missing
sheet_findings.json	no column	Missing
state_compliance.json	no column	Missing
city_compliance.json	no column	Missing
qa_result.json	no column	Missing
What's Good About the Proposed Schema
projects.user_id references auth.users(id) directly -- correct. The old schema had a separate crossbeam.users table. You don't need that anymore.
messages.id is BIGSERIAL -- smart. Enables efficient polling with WHERE id > last_seen_id instead of timestamp ordering. Better than the old UUID approach.
RLS with is_demo = true bypass -- perfect for hackathon. Judges can see demo projects without needing real auth ownership.
Messages INSERT policy is SELECT-only for the frontend -- correct. Server inserts via service role key (bypasses RLS), frontend only polls (SELECT). The missing INSERT policy on messages/outputs is intentional, not a bug.
files table is cleaner than the old client_files -- better naming, type constraints.
My Recommendations
1. Add a catch-all JSONB column to outputs
-- Add to crossbeam.outputs:
raw_artifacts JSONB DEFAULT '{}'::jsonb,
-- Add to crossbeam.outputs:raw_artifacts JSONB DEFAULT '{}'::jsonb,
This lets the server dump ALL the agent's intermediate files as a keyed JSON blob ({"sheet_manifest": {...}, "state_law_findings": {...}, ...}). The frontend can cherry-pick what to display, and you don't lose anything.
2. Rename action_items_json to professional_scope_md
The agent produces a Markdown scope-of-work document, not a JSON action items list. Or keep both and add the MD column.
3. Add response_letter_pdf_path storage path tracking
The plan has this column but the corrections flow also generates PDFs during the response phase. Make sure the server knows which flow outputs map to which columns.
4. Consider a flow_phase or pass_number on outputs
The corrections flow runs in two phases (analysis then response). The current schema has a version field which could work, but something like flow_phase TEXT CHECK (flow_phase IN ('analysis', 'response', 'review')) would be more explicit.
5. The corrections-response flow has no flow_type
The projects.flow_type allows 'city-review' and 'corrections-analysis'. But the agent SDK has THREE flows: plan-review.ts, corrections-analysis.ts, and corrections-response.ts. The response flow is a follow-up to analysis. You'll need to decide: does it reuse the same project (with version incrementing), or is it a separate flow type?
Bottom Line
For a hackathon demo where you're showing the key deliverables (corrections letter, checklist, response letter), the schema works. The server layer just needs to cherry-pick the right files from the agent's output directory and map them to columns. Adding raw_artifacts JSONB gives you a safety net for everything else.
Want me to go ahead and execute Stream 0 -- nuke the old schema and create the new one with these fixes applied?